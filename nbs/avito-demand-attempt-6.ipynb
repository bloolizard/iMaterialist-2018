{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.preprocessing import CategoricalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torchvision.models as models\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os.path as dpath\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "np.set_printoptions(threshold=50, edgeitems=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/edwin/Datasets/competitions/avito-demand-prediction/\"\n",
    "PATH = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_CSV = pd.read_csv(PATH/'train.csv')\n",
    "TRN_CSV_COPY = TRN_CSV.copy()\n",
    "\n",
    "TEST_CSV = pd.read_csv(PATH/'test.csv')\n",
    "TEST_CSV_COPY = TEST_CSV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['item_id', 'user_id', 'region', 'city', 'parent_category_name', 'category_name', \n",
    "           'param_1', 'param_2', 'param_3', 'title', 'description', 'user_type']\n",
    "cont_vars = ['price', 'item_seq_number', 'deal_probability']\n",
    "cont_vars_test = ['price', 'item_seq_number']\n",
    "skip_vars = ['activation_date', 'image']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503424"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(TRN_CSV_COPY); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = len(TEST_CSV_COPY); n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_COPY['deal_probability'] = [0 for i in range(n_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_vars:\n",
    "    TRN_CSV_COPY[c] = TRN_CSV_COPY[c].astype(\"category\").cat.as_ordered()\n",
    "    TEST_CSV_COPY[c] = TEST_CSV_COPY[c].astype(\"category\").cat.as_ordered()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cont_vars:\n",
    "    TRN_CSV_COPY[c] = TRN_CSV_COPY[c].astype(\"float32\")\n",
    "    TEST_CSV_COPY[c] = TEST_CSV_COPY[c].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_cv_idxs(n, val_pct=15000/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_SAMPLE = TRN_CSV_COPY.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = len(TRN_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11250"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(n_sample * train_ratio); train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y, nas, mapper = proc_df(TRN_SAMPLE, 'deal_probability', do_scale=True, skip_flds=skip_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>price_na</th>\n",
       "      <th>image_top_1_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143309</th>\n",
       "      <td>1111769</td>\n",
       "      <td>325705</td>\n",
       "      <td>11</td>\n",
       "      <td>744</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>685435</td>\n",
       "      <td>1083896</td>\n",
       "      <td>-0.046974</td>\n",
       "      <td>-0.115768</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.034376</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771572</th>\n",
       "      <td>293470</td>\n",
       "      <td>460746</td>\n",
       "      <td>28</td>\n",
       "      <td>1725</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109900</td>\n",
       "      <td>601767</td>\n",
       "      <td>-0.053180</td>\n",
       "      <td>-0.061905</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.453284</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378472</th>\n",
       "      <td>1277167</td>\n",
       "      <td>345127</td>\n",
       "      <td>24</td>\n",
       "      <td>1542</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>207</td>\n",
       "      <td>201</td>\n",
       "      <td>164</td>\n",
       "      <td>501637</td>\n",
       "      <td>1047537</td>\n",
       "      <td>-0.057483</td>\n",
       "      <td>-0.129318</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.605013</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846158</th>\n",
       "      <td>1129361</td>\n",
       "      <td>183646</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>193</td>\n",
       "      <td>105</td>\n",
       "      <td>637144</td>\n",
       "      <td>1004675</td>\n",
       "      <td>-0.057870</td>\n",
       "      <td>-0.032603</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.211932</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84567</th>\n",
       "      <td>896590</td>\n",
       "      <td>767860</td>\n",
       "      <td>10</td>\n",
       "      <td>1043</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>203</td>\n",
       "      <td>139</td>\n",
       "      <td>784059</td>\n",
       "      <td>1183459</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>-0.127963</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.222693</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984506</th>\n",
       "      <td>1447298</td>\n",
       "      <td>426696</td>\n",
       "      <td>17</td>\n",
       "      <td>1277</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>203</td>\n",
       "      <td>139</td>\n",
       "      <td>445539</td>\n",
       "      <td>572608</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.119833</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.875113</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276450</th>\n",
       "      <td>1309392</td>\n",
       "      <td>588499</td>\n",
       "      <td>24</td>\n",
       "      <td>1542</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>410853</td>\n",
       "      <td>1055306</td>\n",
       "      <td>-0.057041</td>\n",
       "      <td>5.498986</td>\n",
       "      <td>3</td>\n",
       "      <td>1.856020</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063503</th>\n",
       "      <td>74820</td>\n",
       "      <td>163965</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84109</td>\n",
       "      <td>974463</td>\n",
       "      <td>-0.057815</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>1</td>\n",
       "      <td>1.128579</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170088</th>\n",
       "      <td>1254217</td>\n",
       "      <td>741249</td>\n",
       "      <td>15</td>\n",
       "      <td>1108</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>2</td>\n",
       "      <td>1174</td>\n",
       "      <td>14642</td>\n",
       "      <td>115724</td>\n",
       "      <td>0.305894</td>\n",
       "      <td>-0.131520</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125657</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334881</th>\n",
       "      <td>454169</td>\n",
       "      <td>226900</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>232</td>\n",
       "      <td>144</td>\n",
       "      <td>642211</td>\n",
       "      <td>993559</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.119664</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.709395</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492727</th>\n",
       "      <td>619149</td>\n",
       "      <td>403477</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>743876</td>\n",
       "      <td>852103</td>\n",
       "      <td>-0.056708</td>\n",
       "      <td>-0.130504</td>\n",
       "      <td>2</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435330</th>\n",
       "      <td>71076</td>\n",
       "      <td>670539</td>\n",
       "      <td>10</td>\n",
       "      <td>724</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133588</td>\n",
       "      <td>91471</td>\n",
       "      <td>-0.058003</td>\n",
       "      <td>-0.128472</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.473729</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219515</th>\n",
       "      <td>376077</td>\n",
       "      <td>315503</td>\n",
       "      <td>18</td>\n",
       "      <td>1301</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307716</td>\n",
       "      <td>316112</td>\n",
       "      <td>-0.057195</td>\n",
       "      <td>0.086471</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.231608</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308138</th>\n",
       "      <td>1095465</td>\n",
       "      <td>3853</td>\n",
       "      <td>22</td>\n",
       "      <td>586</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60297</td>\n",
       "      <td>508522</td>\n",
       "      <td>-0.057483</td>\n",
       "      <td>-0.123560</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.479110</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833263</th>\n",
       "      <td>368744</td>\n",
       "      <td>171632</td>\n",
       "      <td>22</td>\n",
       "      <td>586</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>719425</td>\n",
       "      <td>845689</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.127286</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.173498</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>3.414415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466866</th>\n",
       "      <td>1280269</td>\n",
       "      <td>502527</td>\n",
       "      <td>17</td>\n",
       "      <td>1458</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>625782</td>\n",
       "      <td>1288927</td>\n",
       "      <td>-0.057826</td>\n",
       "      <td>-0.128980</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.195020</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954186</th>\n",
       "      <td>715882</td>\n",
       "      <td>244042</td>\n",
       "      <td>20</td>\n",
       "      <td>606</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>324682</td>\n",
       "      <td>1031457</td>\n",
       "      <td>-0.056708</td>\n",
       "      <td>-0.131012</td>\n",
       "      <td>2</td>\n",
       "      <td>1.697834</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467040</th>\n",
       "      <td>802784</td>\n",
       "      <td>272602</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>658739</td>\n",
       "      <td>828901</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.118309</td>\n",
       "      <td>1</td>\n",
       "      <td>1.852792</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841485</th>\n",
       "      <td>1409875</td>\n",
       "      <td>563736</td>\n",
       "      <td>22</td>\n",
       "      <td>586</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>251</td>\n",
       "      <td>16</td>\n",
       "      <td>739520</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.127794</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.194714</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387023</th>\n",
       "      <td>1277490</td>\n",
       "      <td>276669</td>\n",
       "      <td>11</td>\n",
       "      <td>744</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91081</td>\n",
       "      <td>27629</td>\n",
       "      <td>-0.056045</td>\n",
       "      <td>-0.130843</td>\n",
       "      <td>2</td>\n",
       "      <td>1.807596</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738740</th>\n",
       "      <td>223696</td>\n",
       "      <td>441731</td>\n",
       "      <td>12</td>\n",
       "      <td>1127</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>468404</td>\n",
       "      <td>704265</td>\n",
       "      <td>-0.057737</td>\n",
       "      <td>-0.131690</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.669579</td>\n",
       "      <td>4.103158</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042955</th>\n",
       "      <td>1369562</td>\n",
       "      <td>437946</td>\n",
       "      <td>10</td>\n",
       "      <td>1388</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537325</td>\n",
       "      <td>611591</td>\n",
       "      <td>-0.058034</td>\n",
       "      <td>-0.127794</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.770732</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68461</th>\n",
       "      <td>917697</td>\n",
       "      <td>581950</td>\n",
       "      <td>26</td>\n",
       "      <td>1248</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>219634</td>\n",
       "      <td>636182</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>-0.131859</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.423153</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996059</th>\n",
       "      <td>990997</td>\n",
       "      <td>629782</td>\n",
       "      <td>5</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>657678</td>\n",
       "      <td>1033708</td>\n",
       "      <td>-0.057682</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.567350</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15498</th>\n",
       "      <td>1304314</td>\n",
       "      <td>166095</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350880</td>\n",
       "      <td>397078</td>\n",
       "      <td>-0.058002</td>\n",
       "      <td>-0.117970</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465121</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354602</th>\n",
       "      <td>753999</td>\n",
       "      <td>405474</td>\n",
       "      <td>21</td>\n",
       "      <td>909</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>127</td>\n",
       "      <td>96</td>\n",
       "      <td>189707</td>\n",
       "      <td>631160</td>\n",
       "      <td>-0.013789</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.123998</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642727</th>\n",
       "      <td>1253945</td>\n",
       "      <td>678072</td>\n",
       "      <td>24</td>\n",
       "      <td>1542</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>122</td>\n",
       "      <td>180</td>\n",
       "      <td>455697</td>\n",
       "      <td>1268270</td>\n",
       "      <td>-0.057638</td>\n",
       "      <td>-0.120341</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.208703</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099928</th>\n",
       "      <td>412548</td>\n",
       "      <td>87563</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>150</td>\n",
       "      <td>296588</td>\n",
       "      <td>432010</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.129996</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.742754</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395782</th>\n",
       "      <td>529942</td>\n",
       "      <td>431862</td>\n",
       "      <td>10</td>\n",
       "      <td>1388</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>186</td>\n",
       "      <td>393925</td>\n",
       "      <td>77638</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.131182</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.195790</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966568</th>\n",
       "      <td>708684</td>\n",
       "      <td>476625</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>106</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>675529</td>\n",
       "      <td>957875</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.124745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831576</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653352</th>\n",
       "      <td>842197</td>\n",
       "      <td>251197</td>\n",
       "      <td>12</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>193</td>\n",
       "      <td>76</td>\n",
       "      <td>561771</td>\n",
       "      <td>741771</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.106791</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.218388</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472013</th>\n",
       "      <td>192203</td>\n",
       "      <td>17984</td>\n",
       "      <td>12</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>193</td>\n",
       "      <td>117</td>\n",
       "      <td>635966</td>\n",
       "      <td>430521</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.129827</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.783645</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201582</th>\n",
       "      <td>118994</td>\n",
       "      <td>720807</td>\n",
       "      <td>2</td>\n",
       "      <td>1590</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347046</td>\n",
       "      <td>518095</td>\n",
       "      <td>-0.057737</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>2</td>\n",
       "      <td>0.912283</td>\n",
       "      <td>4.103158</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627956</th>\n",
       "      <td>1001976</td>\n",
       "      <td>506713</td>\n",
       "      <td>28</td>\n",
       "      <td>1725</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>109</td>\n",
       "      <td>629999</td>\n",
       "      <td>432473</td>\n",
       "      <td>-0.057859</td>\n",
       "      <td>-0.107130</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.262508</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582477</th>\n",
       "      <td>1030744</td>\n",
       "      <td>160764</td>\n",
       "      <td>17</td>\n",
       "      <td>1277</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>1170</td>\n",
       "      <td>158872</td>\n",
       "      <td>1208073</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.129827</td>\n",
       "      <td>2</td>\n",
       "      <td>1.635420</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461125</th>\n",
       "      <td>147236</td>\n",
       "      <td>65480</td>\n",
       "      <td>27</td>\n",
       "      <td>858</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>683483</td>\n",
       "      <td>197396</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.437142</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427767</th>\n",
       "      <td>707409</td>\n",
       "      <td>244153</td>\n",
       "      <td>5</td>\n",
       "      <td>318</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>1174</td>\n",
       "      <td>3030</td>\n",
       "      <td>896</td>\n",
       "      <td>0.236205</td>\n",
       "      <td>-0.121358</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.173498</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>3.414415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994475</th>\n",
       "      <td>1218302</td>\n",
       "      <td>211816</td>\n",
       "      <td>10</td>\n",
       "      <td>724</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>1174</td>\n",
       "      <td>6421</td>\n",
       "      <td>522090</td>\n",
       "      <td>0.483988</td>\n",
       "      <td>1.250445</td>\n",
       "      <td>3</td>\n",
       "      <td>0.227886</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186166</th>\n",
       "      <td>1033626</td>\n",
       "      <td>557578</td>\n",
       "      <td>16</td>\n",
       "      <td>1154</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110616</td>\n",
       "      <td>1033755</td>\n",
       "      <td>-0.057815</td>\n",
       "      <td>-0.124576</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.453284</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101465</th>\n",
       "      <td>278496</td>\n",
       "      <td>217886</td>\n",
       "      <td>18</td>\n",
       "      <td>1301</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>119</td>\n",
       "      <td>49</td>\n",
       "      <td>161542</td>\n",
       "      <td>391886</td>\n",
       "      <td>-0.037019</td>\n",
       "      <td>-0.132029</td>\n",
       "      <td>2</td>\n",
       "      <td>1.126427</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028107</th>\n",
       "      <td>120509</td>\n",
       "      <td>426871</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>443803</td>\n",
       "      <td>177238</td>\n",
       "      <td>-0.054717</td>\n",
       "      <td>-0.131012</td>\n",
       "      <td>2</td>\n",
       "      <td>0.171929</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28471</th>\n",
       "      <td>1501313</td>\n",
       "      <td>321070</td>\n",
       "      <td>10</td>\n",
       "      <td>1355</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>106</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>675961</td>\n",
       "      <td>1077619</td>\n",
       "      <td>-0.057262</td>\n",
       "      <td>-0.107807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834804</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305140</th>\n",
       "      <td>980578</td>\n",
       "      <td>720606</td>\n",
       "      <td>20</td>\n",
       "      <td>148</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>258</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>202749</td>\n",
       "      <td>652460</td>\n",
       "      <td>0.085766</td>\n",
       "      <td>-0.132198</td>\n",
       "      <td>2</td>\n",
       "      <td>0.110591</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342052</th>\n",
       "      <td>1109030</td>\n",
       "      <td>399289</td>\n",
       "      <td>2</td>\n",
       "      <td>1590</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400490</td>\n",
       "      <td>771346</td>\n",
       "      <td>-0.057903</td>\n",
       "      <td>-0.129657</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.217618</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040721</th>\n",
       "      <td>313116</td>\n",
       "      <td>635331</td>\n",
       "      <td>9</td>\n",
       "      <td>646</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51298</td>\n",
       "      <td>841360</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.122543</td>\n",
       "      <td>2</td>\n",
       "      <td>1.539648</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954445</th>\n",
       "      <td>822202</td>\n",
       "      <td>395340</td>\n",
       "      <td>18</td>\n",
       "      <td>1301</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364789</td>\n",
       "      <td>169788</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.120172</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.040062</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381584</th>\n",
       "      <td>1135281</td>\n",
       "      <td>492171</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>101</td>\n",
       "      <td>1144</td>\n",
       "      <td>96284</td>\n",
       "      <td>658547</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>-0.130673</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.119694</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159959</th>\n",
       "      <td>842824</td>\n",
       "      <td>401195</td>\n",
       "      <td>12</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>17</td>\n",
       "      <td>507236</td>\n",
       "      <td>893847</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.120680</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.880494</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615307</th>\n",
       "      <td>38805</td>\n",
       "      <td>522976</td>\n",
       "      <td>4</td>\n",
       "      <td>1436</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>122</td>\n",
       "      <td>180</td>\n",
       "      <td>312588</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>-0.117801</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.208703</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256011</th>\n",
       "      <td>458926</td>\n",
       "      <td>170142</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>203</td>\n",
       "      <td>153</td>\n",
       "      <td>507236</td>\n",
       "      <td>1169206</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.126778</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.583491</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245066</th>\n",
       "      <td>436908</td>\n",
       "      <td>750116</td>\n",
       "      <td>6</td>\n",
       "      <td>330</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>331979</td>\n",
       "      <td>120743</td>\n",
       "      <td>-0.057952</td>\n",
       "      <td>-0.120849</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.469425</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791408</th>\n",
       "      <td>1030571</td>\n",
       "      <td>185501</td>\n",
       "      <td>25</td>\n",
       "      <td>546</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>122</td>\n",
       "      <td>186</td>\n",
       "      <td>571097</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057914</td>\n",
       "      <td>-0.127963</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.208703</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123135</th>\n",
       "      <td>778921</td>\n",
       "      <td>321543</td>\n",
       "      <td>19</td>\n",
       "      <td>1306</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>669985</td>\n",
       "      <td>539437</td>\n",
       "      <td>-0.056709</td>\n",
       "      <td>-0.062752</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058939</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515004</th>\n",
       "      <td>1098373</td>\n",
       "      <td>312180</td>\n",
       "      <td>4</td>\n",
       "      <td>347</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>251</td>\n",
       "      <td>15</td>\n",
       "      <td>739554</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.127794</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.246367</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743964</th>\n",
       "      <td>609788</td>\n",
       "      <td>651228</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>204</td>\n",
       "      <td>510632</td>\n",
       "      <td>969812</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>-0.132029</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.193638</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307669</th>\n",
       "      <td>308066</td>\n",
       "      <td>131645</td>\n",
       "      <td>27</td>\n",
       "      <td>1633</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>144</td>\n",
       "      <td>373693</td>\n",
       "      <td>765188</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.131012</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.741677</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668633</th>\n",
       "      <td>1360600</td>\n",
       "      <td>21131</td>\n",
       "      <td>17</td>\n",
       "      <td>1277</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>76</td>\n",
       "      <td>317</td>\n",
       "      <td>70990</td>\n",
       "      <td>418512</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>-0.130335</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.106780</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88877</th>\n",
       "      <td>915146</td>\n",
       "      <td>116544</td>\n",
       "      <td>16</td>\n",
       "      <td>1154</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>686776</td>\n",
       "      <td>1084649</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.118139</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.961201</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058179</th>\n",
       "      <td>848662</td>\n",
       "      <td>164063</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81353</td>\n",
       "      <td>442911</td>\n",
       "      <td>-0.050956</td>\n",
       "      <td>-0.124068</td>\n",
       "      <td>3</td>\n",
       "      <td>1.808672</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808649</th>\n",
       "      <td>608985</td>\n",
       "      <td>24637</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>193</td>\n",
       "      <td>27</td>\n",
       "      <td>412105</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.132198</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.211932</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  user_id  region  city  parent_category_name  category_name  \\\n",
       "143309   1111769   325705      11   744                     5             30   \n",
       "771572    293470   460746      28  1725                     5             43   \n",
       "1378472  1277167   345127      24  1542                     5             30   \n",
       "846158   1129361   183646      13  1049                     5             11   \n",
       "84567     896590   767860      10  1043                     5             30   \n",
       "984506   1447298   426696      17  1277                     5             30   \n",
       "1276450  1309392   588499      24  1542                     1             42   \n",
       "1063503    74820   163965      20   461                     5             30   \n",
       "170088   1254217   741249      15  1108                     6             16   \n",
       "1334881   454169   226900      20   461                     5             30   \n",
       "1492727   619149   403477      13  1049                     3              5   \n",
       "1435330    71076   670539      10   724                     5             22   \n",
       "1219515   376077   315503      18  1301                     5             43   \n",
       "308138   1095465     3853      22   586                     5             22   \n",
       "833263    368744   171632      22   586                     3             39   \n",
       "1466866  1280269   502527      17  1458                     5             43   \n",
       "954186    715882   244042      20   606                     1             26   \n",
       "1467040   802784   272602      20   461                     1             42   \n",
       "841485   1409875   563736      22   586                     5             11   \n",
       "387023   1277490   276669      11   744                     1             15   \n",
       "738740    223696   441731      12  1127                     5             11   \n",
       "1042955  1369562   437946      10  1388                     8             35   \n",
       "68461     917697   581950      26  1248                     5             43   \n",
       "996059    990997   629782       5   318                     5             22   \n",
       "15498    1304314   166095      13  1049                     5             22   \n",
       "1354602   753999   405474      21   909                     7              1   \n",
       "642727   1253945   678072      24  1542                     5             11   \n",
       "1099928   412548    87563      20   461                     5             30   \n",
       "395782    529942   431862      10  1388                     5             11   \n",
       "966568    708684   476625       1   124                     3              5   \n",
       "...          ...      ...     ...   ...                   ...            ...   \n",
       "653352    842197   251197      12   976                     5             11   \n",
       "1472013   192203    17984      12   976                     5             30   \n",
       "1201582   118994   720807       2  1590                     4             21   \n",
       "627956   1001976   506713      28  1725                     5             11   \n",
       "582477   1030744   160764      17  1277                     5             11   \n",
       "461125    147236    65480      27   858                     5             43   \n",
       "1427767   707409   244153       5   318                     6             16   \n",
       "994475   1218302   211816      10   724                     6             16   \n",
       "1186166  1033626   557578      16  1154                     5             43   \n",
       "101465    278496   217886      18  1301                     7              1   \n",
       "1028107   120509   426871      20   461                     3             39   \n",
       "28471    1501313   321070      10  1355                     3              5   \n",
       "1305140   980578   720606      20   148                     6             12   \n",
       "342052   1109030   399289       2  1590                     5             43   \n",
       "1040721   313116   635331       9   646                     9             41   \n",
       "954445    822202   395340      18  1301                     8             35   \n",
       "381584   1135281   492171       3   137                     7              1   \n",
       "1159959   842824   401195      12   976                     5             11   \n",
       "615307     38805   522976       4  1436                     5             11   \n",
       "256011    458926   170142      20   461                     5             30   \n",
       "245066    436908   750116       6   330                     5             22   \n",
       "791408   1030571   185501      25   546                     5             11   \n",
       "123135    778921   321543      19  1306                     9             32   \n",
       "515004   1098373   312180       4   347                     5             11   \n",
       "743964    609788   651228      20   461                     5             11   \n",
       "1307669   308066   131645      27  1633                     5             30   \n",
       "668633   1360600    21131      17  1277                     7              1   \n",
       "88877     915146   116544      16  1154                     9             41   \n",
       "1058179   848662   164063      13  1049                     1             15   \n",
       "808649    608985    24637      13  1049                     5             11   \n",
       "\n",
       "         param_1  param_2  param_3   title  description     price  \\\n",
       "143309        41        0        0  685435      1083896 -0.046974   \n",
       "771572        38        0        0  109900       601767 -0.053180   \n",
       "1378472      207      201      164  501637      1047537 -0.057483   \n",
       "846158       105      193      105  637144      1004675 -0.057870   \n",
       "84567        126      203      139  784059      1183459 -0.057992   \n",
       "984506       126      203      139  445539       572608 -0.057704   \n",
       "1276450       13        0        0  410853      1055306 -0.057041   \n",
       "1063503       41        0        0   84109       974463 -0.057815   \n",
       "170088       258        2     1174   14642       115724  0.305894   \n",
       "1334881      126      232      144  642211       993559 -0.057593   \n",
       "1492727      108      261        0  743876       852103 -0.056708   \n",
       "1435330      172        0        0  133588        91471 -0.058003   \n",
       "1219515      102        0        0  307716       316112 -0.057195   \n",
       "308138       233        0        0   60297       508522 -0.057483   \n",
       "833263       311        0        0  719425       845689 -0.057970   \n",
       "1466866      325        0        0  625782      1288927 -0.057826   \n",
       "954186         0        0        0  324682      1031457 -0.056708   \n",
       "1467040       21        0        0  658739       828901 -0.057704   \n",
       "841485       110      251       16  739520            0 -0.058014   \n",
       "387023       133        0        0   91081        27629 -0.056045   \n",
       "738740       105      142        0  468404       704265 -0.057737   \n",
       "1042955      344        0        0  537325       611591 -0.058034   \n",
       "68461        322        0        0  219634       636182 -0.057372   \n",
       "996059       254        0        0  657678      1033708 -0.057682   \n",
       "15498        172        0        0  350880       397078 -0.058002   \n",
       "1354602      278      127       96  189707       631160 -0.013789   \n",
       "642727       105      122      180  455697      1268270 -0.057638   \n",
       "1099928      126      122      150  296588       432010 -0.057593   \n",
       "395782       105      203      186  393925        77638 -0.057970   \n",
       "966568       106      240        0  675529       957875 -0.057593   \n",
       "...          ...      ...      ...     ...          ...       ...   \n",
       "653352       105      193       76  561771       741771 -0.058014   \n",
       "1472013      126      193      117  635966       430521 -0.057704   \n",
       "1201582      120        0        0  347046       518095 -0.057737   \n",
       "627956       110      193      109  629999       432473 -0.057859   \n",
       "582477       110      193     1170  158872      1208073 -0.057704   \n",
       "461125       322        0        0  683483       197396 -0.057593   \n",
       "1427767      258        1     1174    3030          896  0.236205   \n",
       "994475       258        1     1174    6421       522090  0.483988   \n",
       "1186166       38        0        0  110616      1033755 -0.057815   \n",
       "101465       278      119       49  161542       391886 -0.037019   \n",
       "1028107       97        0        0  443803       177238 -0.054717   \n",
       "28471        106      240        0  675961      1077619 -0.057262   \n",
       "1305140      258      134        0  202749       652460  0.085766   \n",
       "342052       249        0        0  400490       771346 -0.057903   \n",
       "1040721      341        0        0   51298       841360 -0.057704   \n",
       "954445       241        0        0  364789       169788 -0.058014   \n",
       "381584       278      101     1144   96284       658547  0.014971   \n",
       "1159959      105      203       17  507236       893847 -0.057925   \n",
       "615307       110      122      180  312588            0 -0.057372   \n",
       "256011       126      203      153  507236      1169206 -0.057593   \n",
       "245066       172        0        0  331979       120743 -0.057952   \n",
       "791408       105      122      186  571097            0 -0.057914   \n",
       "123135         0        0        0  669985       539437 -0.056709   \n",
       "515004       110      251       15  739554            0 -0.058014   \n",
       "743964       105      203      204  510632       969812 -0.057992   \n",
       "1307669      126      122      144  373693       765188 -0.057925   \n",
       "668633       278       76      317   70990       418512  0.003910   \n",
       "88877        122        0        0  686776      1084649 -0.057704   \n",
       "1058179      133        0        0   81353       442911 -0.050956   \n",
       "808649       105      193       27  412105            0 -0.057925   \n",
       "\n",
       "         item_seq_number  user_type  image_top_1  price_na  image_top_1_na  \n",
       "143309         -0.115768          2    -1.034376 -0.243715       -0.292876  \n",
       "771572         -0.061905          1    -0.453284 -0.243715       -0.292876  \n",
       "1378472        -0.129318          2    -0.605013 -0.243715       -0.292876  \n",
       "846158         -0.032603          1    -1.211932 -0.243715       -0.292876  \n",
       "84567          -0.127963          2    -1.222693 -0.243715       -0.292876  \n",
       "984506         -0.119833          2    -0.875113 -0.243715       -0.292876  \n",
       "1276450         5.498986          3     1.856020 -0.243715       -0.292876  \n",
       "1063503        -0.132367          1     1.128579 -0.243715       -0.292876  \n",
       "170088         -0.131520          2     0.125657 -0.243715       -0.292876  \n",
       "1334881        -0.119664          1    -0.709395 -0.243715       -0.292876  \n",
       "1492727        -0.130504          2     0.613128 -0.243715       -0.292876  \n",
       "1435330        -0.128472          2    -0.473729 -0.243715       -0.292876  \n",
       "1219515         0.086471          3    -0.231608 -0.243715       -0.292876  \n",
       "308138         -0.123560          1    -0.479110 -0.243715       -0.292876  \n",
       "833263         -0.127286          2    -0.173498 -0.243715        3.414415  \n",
       "1466866        -0.128980          2    -0.195020 -0.243715       -0.292876  \n",
       "954186         -0.131012          2     1.697834 -0.243715       -0.292876  \n",
       "1467040        -0.118309          1     1.852792 -0.243715       -0.292876  \n",
       "841485         -0.127794          2    -1.194714 -0.243715       -0.292876  \n",
       "387023         -0.130843          2     1.807596 -0.243715       -0.292876  \n",
       "738740         -0.131690          2    -0.669579  4.103158       -0.292876  \n",
       "1042955        -0.127794          2    -0.770732 -0.243715       -0.292876  \n",
       "68461          -0.131859          2    -0.423153 -0.243715       -0.292876  \n",
       "996059         -0.132367          2    -0.567350 -0.243715       -0.292876  \n",
       "15498          -0.117970          3    -0.465121 -0.243715       -0.292876  \n",
       "1354602        -0.132367          2    -0.123998 -0.243715       -0.292876  \n",
       "642727         -0.120341          1    -1.208703 -0.243715       -0.292876  \n",
       "1099928        -0.129996          2    -0.742754 -0.243715       -0.292876  \n",
       "395782         -0.131182          2    -1.195790 -0.243715       -0.292876  \n",
       "966568         -0.124745          1     0.831576 -0.243715       -0.292876  \n",
       "...                  ...        ...          ...       ...             ...  \n",
       "653352         -0.106791          2    -1.218388 -0.243715       -0.292876  \n",
       "1472013        -0.129827          2    -0.783645 -0.243715       -0.292876  \n",
       "1201582        -0.122882          2     0.912283  4.103158       -0.292876  \n",
       "627956         -0.107130          2    -1.262508 -0.243715       -0.292876  \n",
       "582477         -0.129827          2     1.635420 -0.243715       -0.292876  \n",
       "461125         -0.132367          2    -0.437142 -0.243715       -0.292876  \n",
       "1427767        -0.121358          1    -0.173498 -0.243715        3.414415  \n",
       "994475          1.250445          3     0.227886 -0.243715       -0.292876  \n",
       "1186166        -0.124576          2    -0.453284 -0.243715       -0.292876  \n",
       "101465         -0.132029          2     1.126427 -0.243715       -0.292876  \n",
       "1028107        -0.131012          2     0.171929 -0.243715       -0.292876  \n",
       "28471          -0.107807          1     0.834804 -0.243715       -0.292876  \n",
       "1305140        -0.132198          2     0.110591 -0.243715       -0.292876  \n",
       "342052         -0.129657          2    -0.217618 -0.243715       -0.292876  \n",
       "1040721        -0.122543          2     1.539648 -0.243715       -0.292876  \n",
       "954445         -0.120172          2    -0.040062 -0.243715       -0.292876  \n",
       "381584         -0.130673          2    -0.119694 -0.243715       -0.292876  \n",
       "1159959        -0.120680          2    -0.880494 -0.243715       -0.292876  \n",
       "615307         -0.117801          1    -1.208703 -0.243715       -0.292876  \n",
       "256011         -0.126778          2    -0.583491 -0.243715       -0.292876  \n",
       "245066         -0.120849          3    -0.469425 -0.243715       -0.292876  \n",
       "791408         -0.127963          2    -1.208703 -0.243715       -0.292876  \n",
       "123135         -0.062752          1     0.058939 -0.243715       -0.292876  \n",
       "515004         -0.127794          2    -1.246367 -0.243715       -0.292876  \n",
       "743964         -0.132029          2    -1.193638 -0.243715       -0.292876  \n",
       "1307669        -0.131012          2    -0.741677 -0.243715       -0.292876  \n",
       "668633         -0.130335          2    -0.106780 -0.243715       -0.292876  \n",
       "88877          -0.118139          1    -0.961201 -0.243715       -0.292876  \n",
       "1058179        -0.124068          3     1.808672 -0.243715       -0.292876  \n",
       "808649         -0.132198          2    -1.211932 -0.243715       -0.292876  \n",
       "\n",
       "[15000 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = list(range(train_size, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, _, nas, mapper = proc_df(TEST_CSV_COPY, 'deal_probability', do_scale=True, \n",
    "                                  skip_flds=skip_vars, mapper=mapper, na_dict=nas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508438"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(path, val_idxs, df, y.astype(np.float32), \n",
    "                                       cat_flds=cat_vars, bs=4, test_df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('item_id', 1503425),\n",
       " ('user_id', 771770),\n",
       " ('region', 29),\n",
       " ('city', 1734),\n",
       " ('parent_category_name', 10),\n",
       " ('category_name', 48),\n",
       " ('param_1', 372),\n",
       " ('param_2', 272),\n",
       " ('param_3', 1220),\n",
       " ('title', 788378),\n",
       " ('description', 1317103),\n",
       " ('user_type', 4)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_sz = [(c, len(TRN_CSV_COPY[c].cat.categories)+1) for c in cat_vars]; cat_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1503425, 50),\n",
       " (771770, 50),\n",
       " (29, 15),\n",
       " (1734, 50),\n",
       " (10, 5),\n",
       " (48, 24),\n",
       " (372, 50),\n",
       " (272, 50),\n",
       " (1220, 50),\n",
       " (788378, 50),\n",
       " (1317103, 50),\n",
       " (4, 2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for _, c in cat_sz]; emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_y = np.max(y); max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>price_na</th>\n",
       "      <th>image_top_1_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143309</th>\n",
       "      <td>1111769</td>\n",
       "      <td>325705</td>\n",
       "      <td>11</td>\n",
       "      <td>744</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>685435</td>\n",
       "      <td>1083896</td>\n",
       "      <td>-0.046974</td>\n",
       "      <td>-0.115768</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.034376</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771572</th>\n",
       "      <td>293470</td>\n",
       "      <td>460746</td>\n",
       "      <td>28</td>\n",
       "      <td>1725</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109900</td>\n",
       "      <td>601767</td>\n",
       "      <td>-0.053180</td>\n",
       "      <td>-0.061905</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.453284</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378472</th>\n",
       "      <td>1277167</td>\n",
       "      <td>345127</td>\n",
       "      <td>24</td>\n",
       "      <td>1542</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>207</td>\n",
       "      <td>201</td>\n",
       "      <td>164</td>\n",
       "      <td>501637</td>\n",
       "      <td>1047537</td>\n",
       "      <td>-0.057483</td>\n",
       "      <td>-0.129318</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.605013</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846158</th>\n",
       "      <td>1129361</td>\n",
       "      <td>183646</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>193</td>\n",
       "      <td>105</td>\n",
       "      <td>637144</td>\n",
       "      <td>1004675</td>\n",
       "      <td>-0.057870</td>\n",
       "      <td>-0.032603</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.211932</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84567</th>\n",
       "      <td>896590</td>\n",
       "      <td>767860</td>\n",
       "      <td>10</td>\n",
       "      <td>1043</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>203</td>\n",
       "      <td>139</td>\n",
       "      <td>784059</td>\n",
       "      <td>1183459</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>-0.127963</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.222693</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984506</th>\n",
       "      <td>1447298</td>\n",
       "      <td>426696</td>\n",
       "      <td>17</td>\n",
       "      <td>1277</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>203</td>\n",
       "      <td>139</td>\n",
       "      <td>445539</td>\n",
       "      <td>572608</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.119833</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.875113</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276450</th>\n",
       "      <td>1309392</td>\n",
       "      <td>588499</td>\n",
       "      <td>24</td>\n",
       "      <td>1542</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>410853</td>\n",
       "      <td>1055306</td>\n",
       "      <td>-0.057041</td>\n",
       "      <td>5.498986</td>\n",
       "      <td>3</td>\n",
       "      <td>1.856020</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063503</th>\n",
       "      <td>74820</td>\n",
       "      <td>163965</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84109</td>\n",
       "      <td>974463</td>\n",
       "      <td>-0.057815</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>1</td>\n",
       "      <td>1.128579</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170088</th>\n",
       "      <td>1254217</td>\n",
       "      <td>741249</td>\n",
       "      <td>15</td>\n",
       "      <td>1108</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>2</td>\n",
       "      <td>1174</td>\n",
       "      <td>14642</td>\n",
       "      <td>115724</td>\n",
       "      <td>0.305894</td>\n",
       "      <td>-0.131520</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125657</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334881</th>\n",
       "      <td>454169</td>\n",
       "      <td>226900</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>232</td>\n",
       "      <td>144</td>\n",
       "      <td>642211</td>\n",
       "      <td>993559</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.119664</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.709395</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492727</th>\n",
       "      <td>619149</td>\n",
       "      <td>403477</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>743876</td>\n",
       "      <td>852103</td>\n",
       "      <td>-0.056708</td>\n",
       "      <td>-0.130504</td>\n",
       "      <td>2</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435330</th>\n",
       "      <td>71076</td>\n",
       "      <td>670539</td>\n",
       "      <td>10</td>\n",
       "      <td>724</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133588</td>\n",
       "      <td>91471</td>\n",
       "      <td>-0.058003</td>\n",
       "      <td>-0.128472</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.473729</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219515</th>\n",
       "      <td>376077</td>\n",
       "      <td>315503</td>\n",
       "      <td>18</td>\n",
       "      <td>1301</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>307716</td>\n",
       "      <td>316112</td>\n",
       "      <td>-0.057195</td>\n",
       "      <td>0.086471</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.231608</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308138</th>\n",
       "      <td>1095465</td>\n",
       "      <td>3853</td>\n",
       "      <td>22</td>\n",
       "      <td>586</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60297</td>\n",
       "      <td>508522</td>\n",
       "      <td>-0.057483</td>\n",
       "      <td>-0.123560</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.479110</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833263</th>\n",
       "      <td>368744</td>\n",
       "      <td>171632</td>\n",
       "      <td>22</td>\n",
       "      <td>586</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>719425</td>\n",
       "      <td>845689</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.127286</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.173498</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>3.414415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466866</th>\n",
       "      <td>1280269</td>\n",
       "      <td>502527</td>\n",
       "      <td>17</td>\n",
       "      <td>1458</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>625782</td>\n",
       "      <td>1288927</td>\n",
       "      <td>-0.057826</td>\n",
       "      <td>-0.128980</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.195020</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954186</th>\n",
       "      <td>715882</td>\n",
       "      <td>244042</td>\n",
       "      <td>20</td>\n",
       "      <td>606</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>324682</td>\n",
       "      <td>1031457</td>\n",
       "      <td>-0.056708</td>\n",
       "      <td>-0.131012</td>\n",
       "      <td>2</td>\n",
       "      <td>1.697834</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467040</th>\n",
       "      <td>802784</td>\n",
       "      <td>272602</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>658739</td>\n",
       "      <td>828901</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.118309</td>\n",
       "      <td>1</td>\n",
       "      <td>1.852792</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841485</th>\n",
       "      <td>1409875</td>\n",
       "      <td>563736</td>\n",
       "      <td>22</td>\n",
       "      <td>586</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>251</td>\n",
       "      <td>16</td>\n",
       "      <td>739520</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.127794</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.194714</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387023</th>\n",
       "      <td>1277490</td>\n",
       "      <td>276669</td>\n",
       "      <td>11</td>\n",
       "      <td>744</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91081</td>\n",
       "      <td>27629</td>\n",
       "      <td>-0.056045</td>\n",
       "      <td>-0.130843</td>\n",
       "      <td>2</td>\n",
       "      <td>1.807596</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738740</th>\n",
       "      <td>223696</td>\n",
       "      <td>441731</td>\n",
       "      <td>12</td>\n",
       "      <td>1127</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>468404</td>\n",
       "      <td>704265</td>\n",
       "      <td>-0.057737</td>\n",
       "      <td>-0.131690</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.669579</td>\n",
       "      <td>4.103158</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042955</th>\n",
       "      <td>1369562</td>\n",
       "      <td>437946</td>\n",
       "      <td>10</td>\n",
       "      <td>1388</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537325</td>\n",
       "      <td>611591</td>\n",
       "      <td>-0.058034</td>\n",
       "      <td>-0.127794</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.770732</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68461</th>\n",
       "      <td>917697</td>\n",
       "      <td>581950</td>\n",
       "      <td>26</td>\n",
       "      <td>1248</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>219634</td>\n",
       "      <td>636182</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>-0.131859</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.423153</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996059</th>\n",
       "      <td>990997</td>\n",
       "      <td>629782</td>\n",
       "      <td>5</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>657678</td>\n",
       "      <td>1033708</td>\n",
       "      <td>-0.057682</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.567350</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15498</th>\n",
       "      <td>1304314</td>\n",
       "      <td>166095</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350880</td>\n",
       "      <td>397078</td>\n",
       "      <td>-0.058002</td>\n",
       "      <td>-0.117970</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465121</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354602</th>\n",
       "      <td>753999</td>\n",
       "      <td>405474</td>\n",
       "      <td>21</td>\n",
       "      <td>909</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>127</td>\n",
       "      <td>96</td>\n",
       "      <td>189707</td>\n",
       "      <td>631160</td>\n",
       "      <td>-0.013789</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.123998</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642727</th>\n",
       "      <td>1253945</td>\n",
       "      <td>678072</td>\n",
       "      <td>24</td>\n",
       "      <td>1542</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>122</td>\n",
       "      <td>180</td>\n",
       "      <td>455697</td>\n",
       "      <td>1268270</td>\n",
       "      <td>-0.057638</td>\n",
       "      <td>-0.120341</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.208703</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099928</th>\n",
       "      <td>412548</td>\n",
       "      <td>87563</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>150</td>\n",
       "      <td>296588</td>\n",
       "      <td>432010</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.129996</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.742754</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395782</th>\n",
       "      <td>529942</td>\n",
       "      <td>431862</td>\n",
       "      <td>10</td>\n",
       "      <td>1388</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>186</td>\n",
       "      <td>393925</td>\n",
       "      <td>77638</td>\n",
       "      <td>-0.057970</td>\n",
       "      <td>-0.131182</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.195790</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966568</th>\n",
       "      <td>708684</td>\n",
       "      <td>476625</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>106</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>675529</td>\n",
       "      <td>957875</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.124745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831576</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653352</th>\n",
       "      <td>842197</td>\n",
       "      <td>251197</td>\n",
       "      <td>12</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>193</td>\n",
       "      <td>76</td>\n",
       "      <td>561771</td>\n",
       "      <td>741771</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.106791</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.218388</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472013</th>\n",
       "      <td>192203</td>\n",
       "      <td>17984</td>\n",
       "      <td>12</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>193</td>\n",
       "      <td>117</td>\n",
       "      <td>635966</td>\n",
       "      <td>430521</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.129827</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.783645</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201582</th>\n",
       "      <td>118994</td>\n",
       "      <td>720807</td>\n",
       "      <td>2</td>\n",
       "      <td>1590</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347046</td>\n",
       "      <td>518095</td>\n",
       "      <td>-0.057737</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>2</td>\n",
       "      <td>0.912283</td>\n",
       "      <td>4.103158</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627956</th>\n",
       "      <td>1001976</td>\n",
       "      <td>506713</td>\n",
       "      <td>28</td>\n",
       "      <td>1725</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>109</td>\n",
       "      <td>629999</td>\n",
       "      <td>432473</td>\n",
       "      <td>-0.057859</td>\n",
       "      <td>-0.107130</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.262508</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582477</th>\n",
       "      <td>1030744</td>\n",
       "      <td>160764</td>\n",
       "      <td>17</td>\n",
       "      <td>1277</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>1170</td>\n",
       "      <td>158872</td>\n",
       "      <td>1208073</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.129827</td>\n",
       "      <td>2</td>\n",
       "      <td>1.635420</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461125</th>\n",
       "      <td>147236</td>\n",
       "      <td>65480</td>\n",
       "      <td>27</td>\n",
       "      <td>858</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>683483</td>\n",
       "      <td>197396</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.437142</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427767</th>\n",
       "      <td>707409</td>\n",
       "      <td>244153</td>\n",
       "      <td>5</td>\n",
       "      <td>318</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>1174</td>\n",
       "      <td>3030</td>\n",
       "      <td>896</td>\n",
       "      <td>0.236205</td>\n",
       "      <td>-0.121358</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.173498</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>3.414415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994475</th>\n",
       "      <td>1218302</td>\n",
       "      <td>211816</td>\n",
       "      <td>10</td>\n",
       "      <td>724</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>1174</td>\n",
       "      <td>6421</td>\n",
       "      <td>522090</td>\n",
       "      <td>0.483988</td>\n",
       "      <td>1.250445</td>\n",
       "      <td>3</td>\n",
       "      <td>0.227886</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186166</th>\n",
       "      <td>1033626</td>\n",
       "      <td>557578</td>\n",
       "      <td>16</td>\n",
       "      <td>1154</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110616</td>\n",
       "      <td>1033755</td>\n",
       "      <td>-0.057815</td>\n",
       "      <td>-0.124576</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.453284</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101465</th>\n",
       "      <td>278496</td>\n",
       "      <td>217886</td>\n",
       "      <td>18</td>\n",
       "      <td>1301</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>119</td>\n",
       "      <td>49</td>\n",
       "      <td>161542</td>\n",
       "      <td>391886</td>\n",
       "      <td>-0.037019</td>\n",
       "      <td>-0.132029</td>\n",
       "      <td>2</td>\n",
       "      <td>1.126427</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028107</th>\n",
       "      <td>120509</td>\n",
       "      <td>426871</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>443803</td>\n",
       "      <td>177238</td>\n",
       "      <td>-0.054717</td>\n",
       "      <td>-0.131012</td>\n",
       "      <td>2</td>\n",
       "      <td>0.171929</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28471</th>\n",
       "      <td>1501313</td>\n",
       "      <td>321070</td>\n",
       "      <td>10</td>\n",
       "      <td>1355</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>106</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>675961</td>\n",
       "      <td>1077619</td>\n",
       "      <td>-0.057262</td>\n",
       "      <td>-0.107807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834804</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305140</th>\n",
       "      <td>980578</td>\n",
       "      <td>720606</td>\n",
       "      <td>20</td>\n",
       "      <td>148</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>258</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>202749</td>\n",
       "      <td>652460</td>\n",
       "      <td>0.085766</td>\n",
       "      <td>-0.132198</td>\n",
       "      <td>2</td>\n",
       "      <td>0.110591</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342052</th>\n",
       "      <td>1109030</td>\n",
       "      <td>399289</td>\n",
       "      <td>2</td>\n",
       "      <td>1590</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400490</td>\n",
       "      <td>771346</td>\n",
       "      <td>-0.057903</td>\n",
       "      <td>-0.129657</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.217618</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040721</th>\n",
       "      <td>313116</td>\n",
       "      <td>635331</td>\n",
       "      <td>9</td>\n",
       "      <td>646</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51298</td>\n",
       "      <td>841360</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.122543</td>\n",
       "      <td>2</td>\n",
       "      <td>1.539648</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954445</th>\n",
       "      <td>822202</td>\n",
       "      <td>395340</td>\n",
       "      <td>18</td>\n",
       "      <td>1301</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364789</td>\n",
       "      <td>169788</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.120172</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.040062</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381584</th>\n",
       "      <td>1135281</td>\n",
       "      <td>492171</td>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>101</td>\n",
       "      <td>1144</td>\n",
       "      <td>96284</td>\n",
       "      <td>658547</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>-0.130673</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.119694</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159959</th>\n",
       "      <td>842824</td>\n",
       "      <td>401195</td>\n",
       "      <td>12</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>17</td>\n",
       "      <td>507236</td>\n",
       "      <td>893847</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.120680</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.880494</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615307</th>\n",
       "      <td>38805</td>\n",
       "      <td>522976</td>\n",
       "      <td>4</td>\n",
       "      <td>1436</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>122</td>\n",
       "      <td>180</td>\n",
       "      <td>312588</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>-0.117801</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.208703</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256011</th>\n",
       "      <td>458926</td>\n",
       "      <td>170142</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>203</td>\n",
       "      <td>153</td>\n",
       "      <td>507236</td>\n",
       "      <td>1169206</td>\n",
       "      <td>-0.057593</td>\n",
       "      <td>-0.126778</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.583491</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245066</th>\n",
       "      <td>436908</td>\n",
       "      <td>750116</td>\n",
       "      <td>6</td>\n",
       "      <td>330</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>331979</td>\n",
       "      <td>120743</td>\n",
       "      <td>-0.057952</td>\n",
       "      <td>-0.120849</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.469425</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791408</th>\n",
       "      <td>1030571</td>\n",
       "      <td>185501</td>\n",
       "      <td>25</td>\n",
       "      <td>546</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>122</td>\n",
       "      <td>186</td>\n",
       "      <td>571097</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057914</td>\n",
       "      <td>-0.127963</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.208703</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123135</th>\n",
       "      <td>778921</td>\n",
       "      <td>321543</td>\n",
       "      <td>19</td>\n",
       "      <td>1306</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>669985</td>\n",
       "      <td>539437</td>\n",
       "      <td>-0.056709</td>\n",
       "      <td>-0.062752</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058939</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515004</th>\n",
       "      <td>1098373</td>\n",
       "      <td>312180</td>\n",
       "      <td>4</td>\n",
       "      <td>347</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>251</td>\n",
       "      <td>15</td>\n",
       "      <td>739554</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.058014</td>\n",
       "      <td>-0.127794</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.246367</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743964</th>\n",
       "      <td>609788</td>\n",
       "      <td>651228</td>\n",
       "      <td>20</td>\n",
       "      <td>461</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>204</td>\n",
       "      <td>510632</td>\n",
       "      <td>969812</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>-0.132029</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.193638</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307669</th>\n",
       "      <td>308066</td>\n",
       "      <td>131645</td>\n",
       "      <td>27</td>\n",
       "      <td>1633</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>144</td>\n",
       "      <td>373693</td>\n",
       "      <td>765188</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.131012</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.741677</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668633</th>\n",
       "      <td>1360600</td>\n",
       "      <td>21131</td>\n",
       "      <td>17</td>\n",
       "      <td>1277</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>76</td>\n",
       "      <td>317</td>\n",
       "      <td>70990</td>\n",
       "      <td>418512</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>-0.130335</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.106780</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88877</th>\n",
       "      <td>915146</td>\n",
       "      <td>116544</td>\n",
       "      <td>16</td>\n",
       "      <td>1154</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>686776</td>\n",
       "      <td>1084649</td>\n",
       "      <td>-0.057704</td>\n",
       "      <td>-0.118139</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.961201</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058179</th>\n",
       "      <td>848662</td>\n",
       "      <td>164063</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81353</td>\n",
       "      <td>442911</td>\n",
       "      <td>-0.050956</td>\n",
       "      <td>-0.124068</td>\n",
       "      <td>3</td>\n",
       "      <td>1.808672</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808649</th>\n",
       "      <td>608985</td>\n",
       "      <td>24637</td>\n",
       "      <td>13</td>\n",
       "      <td>1049</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>193</td>\n",
       "      <td>27</td>\n",
       "      <td>412105</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.132198</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.211932</td>\n",
       "      <td>-0.243715</td>\n",
       "      <td>-0.292876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  user_id  region  city  parent_category_name  category_name  \\\n",
       "143309   1111769   325705      11   744                     5             30   \n",
       "771572    293470   460746      28  1725                     5             43   \n",
       "1378472  1277167   345127      24  1542                     5             30   \n",
       "846158   1129361   183646      13  1049                     5             11   \n",
       "84567     896590   767860      10  1043                     5             30   \n",
       "984506   1447298   426696      17  1277                     5             30   \n",
       "1276450  1309392   588499      24  1542                     1             42   \n",
       "1063503    74820   163965      20   461                     5             30   \n",
       "170088   1254217   741249      15  1108                     6             16   \n",
       "1334881   454169   226900      20   461                     5             30   \n",
       "1492727   619149   403477      13  1049                     3              5   \n",
       "1435330    71076   670539      10   724                     5             22   \n",
       "1219515   376077   315503      18  1301                     5             43   \n",
       "308138   1095465     3853      22   586                     5             22   \n",
       "833263    368744   171632      22   586                     3             39   \n",
       "1466866  1280269   502527      17  1458                     5             43   \n",
       "954186    715882   244042      20   606                     1             26   \n",
       "1467040   802784   272602      20   461                     1             42   \n",
       "841485   1409875   563736      22   586                     5             11   \n",
       "387023   1277490   276669      11   744                     1             15   \n",
       "738740    223696   441731      12  1127                     5             11   \n",
       "1042955  1369562   437946      10  1388                     8             35   \n",
       "68461     917697   581950      26  1248                     5             43   \n",
       "996059    990997   629782       5   318                     5             22   \n",
       "15498    1304314   166095      13  1049                     5             22   \n",
       "1354602   753999   405474      21   909                     7              1   \n",
       "642727   1253945   678072      24  1542                     5             11   \n",
       "1099928   412548    87563      20   461                     5             30   \n",
       "395782    529942   431862      10  1388                     5             11   \n",
       "966568    708684   476625       1   124                     3              5   \n",
       "...          ...      ...     ...   ...                   ...            ...   \n",
       "653352    842197   251197      12   976                     5             11   \n",
       "1472013   192203    17984      12   976                     5             30   \n",
       "1201582   118994   720807       2  1590                     4             21   \n",
       "627956   1001976   506713      28  1725                     5             11   \n",
       "582477   1030744   160764      17  1277                     5             11   \n",
       "461125    147236    65480      27   858                     5             43   \n",
       "1427767   707409   244153       5   318                     6             16   \n",
       "994475   1218302   211816      10   724                     6             16   \n",
       "1186166  1033626   557578      16  1154                     5             43   \n",
       "101465    278496   217886      18  1301                     7              1   \n",
       "1028107   120509   426871      20   461                     3             39   \n",
       "28471    1501313   321070      10  1355                     3              5   \n",
       "1305140   980578   720606      20   148                     6             12   \n",
       "342052   1109030   399289       2  1590                     5             43   \n",
       "1040721   313116   635331       9   646                     9             41   \n",
       "954445    822202   395340      18  1301                     8             35   \n",
       "381584   1135281   492171       3   137                     7              1   \n",
       "1159959   842824   401195      12   976                     5             11   \n",
       "615307     38805   522976       4  1436                     5             11   \n",
       "256011    458926   170142      20   461                     5             30   \n",
       "245066    436908   750116       6   330                     5             22   \n",
       "791408   1030571   185501      25   546                     5             11   \n",
       "123135    778921   321543      19  1306                     9             32   \n",
       "515004   1098373   312180       4   347                     5             11   \n",
       "743964    609788   651228      20   461                     5             11   \n",
       "1307669   308066   131645      27  1633                     5             30   \n",
       "668633   1360600    21131      17  1277                     7              1   \n",
       "88877     915146   116544      16  1154                     9             41   \n",
       "1058179   848662   164063      13  1049                     1             15   \n",
       "808649    608985    24637      13  1049                     5             11   \n",
       "\n",
       "         param_1  param_2  param_3   title  description     price  \\\n",
       "143309        41        0        0  685435      1083896 -0.046974   \n",
       "771572        38        0        0  109900       601767 -0.053180   \n",
       "1378472      207      201      164  501637      1047537 -0.057483   \n",
       "846158       105      193      105  637144      1004675 -0.057870   \n",
       "84567        126      203      139  784059      1183459 -0.057992   \n",
       "984506       126      203      139  445539       572608 -0.057704   \n",
       "1276450       13        0        0  410853      1055306 -0.057041   \n",
       "1063503       41        0        0   84109       974463 -0.057815   \n",
       "170088       258        2     1174   14642       115724  0.305894   \n",
       "1334881      126      232      144  642211       993559 -0.057593   \n",
       "1492727      108      261        0  743876       852103 -0.056708   \n",
       "1435330      172        0        0  133588        91471 -0.058003   \n",
       "1219515      102        0        0  307716       316112 -0.057195   \n",
       "308138       233        0        0   60297       508522 -0.057483   \n",
       "833263       311        0        0  719425       845689 -0.057970   \n",
       "1466866      325        0        0  625782      1288927 -0.057826   \n",
       "954186         0        0        0  324682      1031457 -0.056708   \n",
       "1467040       21        0        0  658739       828901 -0.057704   \n",
       "841485       110      251       16  739520            0 -0.058014   \n",
       "387023       133        0        0   91081        27629 -0.056045   \n",
       "738740       105      142        0  468404       704265 -0.057737   \n",
       "1042955      344        0        0  537325       611591 -0.058034   \n",
       "68461        322        0        0  219634       636182 -0.057372   \n",
       "996059       254        0        0  657678      1033708 -0.057682   \n",
       "15498        172        0        0  350880       397078 -0.058002   \n",
       "1354602      278      127       96  189707       631160 -0.013789   \n",
       "642727       105      122      180  455697      1268270 -0.057638   \n",
       "1099928      126      122      150  296588       432010 -0.057593   \n",
       "395782       105      203      186  393925        77638 -0.057970   \n",
       "966568       106      240        0  675529       957875 -0.057593   \n",
       "...          ...      ...      ...     ...          ...       ...   \n",
       "653352       105      193       76  561771       741771 -0.058014   \n",
       "1472013      126      193      117  635966       430521 -0.057704   \n",
       "1201582      120        0        0  347046       518095 -0.057737   \n",
       "627956       110      193      109  629999       432473 -0.057859   \n",
       "582477       110      193     1170  158872      1208073 -0.057704   \n",
       "461125       322        0        0  683483       197396 -0.057593   \n",
       "1427767      258        1     1174    3030          896  0.236205   \n",
       "994475       258        1     1174    6421       522090  0.483988   \n",
       "1186166       38        0        0  110616      1033755 -0.057815   \n",
       "101465       278      119       49  161542       391886 -0.037019   \n",
       "1028107       97        0        0  443803       177238 -0.054717   \n",
       "28471        106      240        0  675961      1077619 -0.057262   \n",
       "1305140      258      134        0  202749       652460  0.085766   \n",
       "342052       249        0        0  400490       771346 -0.057903   \n",
       "1040721      341        0        0   51298       841360 -0.057704   \n",
       "954445       241        0        0  364789       169788 -0.058014   \n",
       "381584       278      101     1144   96284       658547  0.014971   \n",
       "1159959      105      203       17  507236       893847 -0.057925   \n",
       "615307       110      122      180  312588            0 -0.057372   \n",
       "256011       126      203      153  507236      1169206 -0.057593   \n",
       "245066       172        0        0  331979       120743 -0.057952   \n",
       "791408       105      122      186  571097            0 -0.057914   \n",
       "123135         0        0        0  669985       539437 -0.056709   \n",
       "515004       110      251       15  739554            0 -0.058014   \n",
       "743964       105      203      204  510632       969812 -0.057992   \n",
       "1307669      126      122      144  373693       765188 -0.057925   \n",
       "668633       278       76      317   70990       418512  0.003910   \n",
       "88877        122        0        0  686776      1084649 -0.057704   \n",
       "1058179      133        0        0   81353       442911 -0.050956   \n",
       "808649       105      193       27  412105            0 -0.057925   \n",
       "\n",
       "         item_seq_number  user_type  image_top_1  price_na  image_top_1_na  \n",
       "143309         -0.115768          2    -1.034376 -0.243715       -0.292876  \n",
       "771572         -0.061905          1    -0.453284 -0.243715       -0.292876  \n",
       "1378472        -0.129318          2    -0.605013 -0.243715       -0.292876  \n",
       "846158         -0.032603          1    -1.211932 -0.243715       -0.292876  \n",
       "84567          -0.127963          2    -1.222693 -0.243715       -0.292876  \n",
       "984506         -0.119833          2    -0.875113 -0.243715       -0.292876  \n",
       "1276450         5.498986          3     1.856020 -0.243715       -0.292876  \n",
       "1063503        -0.132367          1     1.128579 -0.243715       -0.292876  \n",
       "170088         -0.131520          2     0.125657 -0.243715       -0.292876  \n",
       "1334881        -0.119664          1    -0.709395 -0.243715       -0.292876  \n",
       "1492727        -0.130504          2     0.613128 -0.243715       -0.292876  \n",
       "1435330        -0.128472          2    -0.473729 -0.243715       -0.292876  \n",
       "1219515         0.086471          3    -0.231608 -0.243715       -0.292876  \n",
       "308138         -0.123560          1    -0.479110 -0.243715       -0.292876  \n",
       "833263         -0.127286          2    -0.173498 -0.243715        3.414415  \n",
       "1466866        -0.128980          2    -0.195020 -0.243715       -0.292876  \n",
       "954186         -0.131012          2     1.697834 -0.243715       -0.292876  \n",
       "1467040        -0.118309          1     1.852792 -0.243715       -0.292876  \n",
       "841485         -0.127794          2    -1.194714 -0.243715       -0.292876  \n",
       "387023         -0.130843          2     1.807596 -0.243715       -0.292876  \n",
       "738740         -0.131690          2    -0.669579  4.103158       -0.292876  \n",
       "1042955        -0.127794          2    -0.770732 -0.243715       -0.292876  \n",
       "68461          -0.131859          2    -0.423153 -0.243715       -0.292876  \n",
       "996059         -0.132367          2    -0.567350 -0.243715       -0.292876  \n",
       "15498          -0.117970          3    -0.465121 -0.243715       -0.292876  \n",
       "1354602        -0.132367          2    -0.123998 -0.243715       -0.292876  \n",
       "642727         -0.120341          1    -1.208703 -0.243715       -0.292876  \n",
       "1099928        -0.129996          2    -0.742754 -0.243715       -0.292876  \n",
       "395782         -0.131182          2    -1.195790 -0.243715       -0.292876  \n",
       "966568         -0.124745          1     0.831576 -0.243715       -0.292876  \n",
       "...                  ...        ...          ...       ...             ...  \n",
       "653352         -0.106791          2    -1.218388 -0.243715       -0.292876  \n",
       "1472013        -0.129827          2    -0.783645 -0.243715       -0.292876  \n",
       "1201582        -0.122882          2     0.912283  4.103158       -0.292876  \n",
       "627956         -0.107130          2    -1.262508 -0.243715       -0.292876  \n",
       "582477         -0.129827          2     1.635420 -0.243715       -0.292876  \n",
       "461125         -0.132367          2    -0.437142 -0.243715       -0.292876  \n",
       "1427767        -0.121358          1    -0.173498 -0.243715        3.414415  \n",
       "994475          1.250445          3     0.227886 -0.243715       -0.292876  \n",
       "1186166        -0.124576          2    -0.453284 -0.243715       -0.292876  \n",
       "101465         -0.132029          2     1.126427 -0.243715       -0.292876  \n",
       "1028107        -0.131012          2     0.171929 -0.243715       -0.292876  \n",
       "28471          -0.107807          1     0.834804 -0.243715       -0.292876  \n",
       "1305140        -0.132198          2     0.110591 -0.243715       -0.292876  \n",
       "342052         -0.129657          2    -0.217618 -0.243715       -0.292876  \n",
       "1040721        -0.122543          2     1.539648 -0.243715       -0.292876  \n",
       "954445         -0.120172          2    -0.040062 -0.243715       -0.292876  \n",
       "381584         -0.130673          2    -0.119694 -0.243715       -0.292876  \n",
       "1159959        -0.120680          2    -0.880494 -0.243715       -0.292876  \n",
       "615307         -0.117801          1    -1.208703 -0.243715       -0.292876  \n",
       "256011         -0.126778          2    -0.583491 -0.243715       -0.292876  \n",
       "245066         -0.120849          3    -0.469425 -0.243715       -0.292876  \n",
       "791408         -0.127963          2    -1.208703 -0.243715       -0.292876  \n",
       "123135         -0.062752          1     0.058939 -0.243715       -0.292876  \n",
       "515004         -0.127794          2    -1.246367 -0.243715       -0.292876  \n",
       "743964         -0.132029          2    -1.193638 -0.243715       -0.292876  \n",
       "1307669        -0.131012          2    -0.741677 -0.243715       -0.292876  \n",
       "668633         -0.130335          2    -0.106780 -0.243715       -0.292876  \n",
       "88877          -0.118139          1    -0.961201 -0.243715       -0.292876  \n",
       "1058179        -0.124068          3     1.808672 -0.243715       -0.292876  \n",
       "808649         -0.132198          2    -1.211932 -0.243715       -0.292876  \n",
       "\n",
       "[15000 rows x 17 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'user_id', 'region', 'city', 'parent_category_name',\n",
       "       'category_name', 'param_1', 'param_2', 'param_3', 'title',\n",
       "       'description', 'price', 'item_seq_number', 'user_type', 'image_top_1',\n",
       "       'price_na', 'image_top_1_na'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_id',\n",
       " 'user_id',\n",
       " 'region',\n",
       " 'city',\n",
       " 'parent_category_name',\n",
       " 'category_name',\n",
       " 'param_1',\n",
       " 'param_2',\n",
       " 'param_3',\n",
       " 'title',\n",
       " 'description',\n",
       " 'user_type']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)-len(cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1, [1000,500], [0.001, 0.01], y_range=y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc3e075da01444788aa535f1cbfea5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2813 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0510517b8367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/learner.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, wds)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_Finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 160\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "m.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f49bc788e54d34b8ccc94c10babaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2813 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/THCTensorCopy.cu:204",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3b1c17f5ae8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 160\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/column_data.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/column_data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/THCTensorCopy.cu:204"
     ]
    }
   ],
   "source": [
    "m.fit(lr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixedInputModel(\n",
       "  (embs): ModuleList(\n",
       "    (0): Embedding(1503425, 50)\n",
       "    (1): Embedding(771770, 50)\n",
       "    (2): Embedding(29, 15)\n",
       "    (3): Embedding(1734, 50)\n",
       "    (4): Embedding(10, 5)\n",
       "    (5): Embedding(48, 24)\n",
       "    (6): Embedding(372, 50)\n",
       "    (7): Embedding(272, 50)\n",
       "    (8): Embedding(1220, 50)\n",
       "    (9): Embedding(788378, 50)\n",
       "    (10): Embedding(1317103, 50)\n",
       "    (11): Embedding(4, 2)\n",
       "  )\n",
       "  (lins): ModuleList(\n",
       "    (0): Linear(in_features=451, out_features=1000, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (outp): Linear(in_features=500, out_features=1, bias=True)\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (drops): ModuleList(\n",
       "    (0): Dropout(p=0.001)\n",
       "    (1): Dropout(p=0.01)\n",
       "  )\n",
       "  (bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'from_arrays',\n",
       " 'from_data_frame',\n",
       " 'from_data_frames',\n",
       " 'from_dls',\n",
       " 'get_learner',\n",
       " 'is_reg',\n",
       " 'path',\n",
       " 'test_dl',\n",
       " 'test_ds',\n",
       " 'trn_dl',\n",
       " 'trn_ds',\n",
       " 'trn_y',\n",
       " 'val_dl',\n",
       " 'val_ds',\n",
       " 'val_y']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1111769,  325705,      11,     744,       5,      30,      41,       0,       0,  685435, 1083896,\n",
       "             2])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = md.trn_ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cont = md.trn_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat, x_cont, y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([201001, 262939,      5,    307,      5,     11,    108,    177,     63, 181529, 153640,      2]),\n",
       " array([-0.05774, -0.12136,  0.86386,  4.10316, -0.29288], dtype=float32),\n",
       " array([ 0.])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixedInputModel(\n",
       "  (embs): ModuleList(\n",
       "    (0): Embedding(1503425, 50)\n",
       "    (1): Embedding(771770, 50)\n",
       "    (2): Embedding(29, 15)\n",
       "    (3): Embedding(1734, 50)\n",
       "    (4): Embedding(10, 5)\n",
       "    (5): Embedding(48, 24)\n",
       "    (6): Embedding(372, 50)\n",
       "    (7): Embedding(272, 50)\n",
       "    (8): Embedding(1220, 50)\n",
       "    (9): Embedding(788378, 50)\n",
       "    (10): Embedding(1317103, 50)\n",
       "    (11): Embedding(4, 2)\n",
       "  )\n",
       "  (lins): ModuleList(\n",
       "    (0): Linear(in_features=451, out_features=1000, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (outp): Linear(in_features=500, out_features=1, bias=True)\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (drops): ModuleList(\n",
       "    (0): Dropout(p=0.001)\n",
       "    (1): Dropout(p=0.01)\n",
       "  )\n",
       "  (bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1118e+06\n",
       " 3.2570e+05\n",
       " 1.1000e+01\n",
       " 7.4400e+02\n",
       " 5.0000e+00\n",
       " 3.0000e+01\n",
       " 4.1000e+01\n",
       " 0.0000e+00\n",
       " 0.0000e+00\n",
       " 6.8544e+05\n",
       " 1.0839e+06\n",
       " 2.0000e+00\n",
       "[torch.LongTensor of size 12]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(torch.from_numpy(x_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.0470\n",
       "-0.1158\n",
       "-1.0344\n",
       "-0.2437\n",
       "-0.2929\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(torch.from_numpy(x_cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-35fe3a67a523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/column_data.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/fastai/column_data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)"
     ]
    }
   ],
   "source": [
    "m.model(Variable(x_cat), Variable(x_cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " \n",
       " Columns 0 to 5 \n",
       "  4.4040e+05  7.2348e+05  1.7000e+01  1.2770e+03  1.0000e+00  4.5000e+01\n",
       "  1.3917e+06  2.2322e+05  1.3000e+01  1.0490e+03  1.0000e+00  3.1000e+01\n",
       "  3.9159e+05  2.2800e+05  2.7000e+01  1.6330e+03  1.0000e+00  2.8000e+01\n",
       "  1.4017e+06  1.8173e+05  2.3000e+01  1.6930e+03  5.0000e+00  3.0000e+01\n",
       " \n",
       " Columns 6 to 11 \n",
       "  7.8000e+01  0.0000e+00  0.0000e+00  1.6410e+05  7.0061e+05  2.0000e+00\n",
       "  2.6500e+02  1.5500e+02  0.0000e+00  2.8192e+05  1.2626e+06  2.0000e+00\n",
       "  0.0000e+00  0.0000e+00  0.0000e+00  4.3904e+05  9.5230e+05  1.0000e+00\n",
       "  1.2600e+02  2.3200e+02  1.4400e+02  6.4227e+05  1.1516e+06  2.0000e+00\n",
       " [torch.LongTensor of size 4x12], \n",
       " -0.0580 -0.1178  1.7398 -0.2437 -0.2929\n",
       " -0.0578 -0.1305  1.6806 -0.2437 -0.2929\n",
       " -0.0573 -0.1181  1.8732 -0.2437 -0.2929\n",
       " -0.0573 -0.1312 -0.7072 -0.2437 -0.2929\n",
       " [torch.FloatTensor of size 4x5], \n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.3246\n",
       "  0.0000\n",
       " [torch.FloatTensor of size 4x1]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat, x_cont, y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 5 \n",
       " 7.2742e+05  5.2266e+05  2.2000e+01  5.8600e+02  8.0000e+00  3.5000e+01\n",
       " 9.9214e+05  1.2332e+05  2.0000e+01  4.6100e+02  5.0000e+00  3.0000e+01\n",
       " 3.3169e+05  5.6759e+05  1.6000e+01  1.1540e+03  6.0000e+00  1.4000e+01\n",
       " 1.2928e+06  1.0392e+05  1.0000e+00  1.2400e+02  5.0000e+00  3.0000e+01\n",
       "\n",
       "Columns 6 to 11 \n",
       " 2.6900e+02  2.6800e+02  0.0000e+00  2.5622e+05  1.2115e+06  2.0000e+00\n",
       " 1.2600e+02  1.9300e+02  1.1700e+02  5.5357e+05  5.4308e+05  2.0000e+00\n",
       " 2.5800e+02  2.0800e+02  0.0000e+00  7.2841e+05  1.1293e+06  1.0000e+00\n",
       " 1.2600e+02  1.9300e+02  1.1500e+02  2.0214e+05  7.3034e+05  2.0000e+00\n",
       "[torch.LongTensor of size 4x12]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0580 -0.1317  1.6634 -0.2437 -0.2929\n",
       "-0.0579 -0.1308 -0.7836 -0.2437 -0.2929\n",
       "-0.0523 -0.1200 -0.1735 -0.2437  3.4144\n",
       "-0.0576 -0.1015 -0.7836 -0.2437 -0.2929\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-02 *\n",
       "  0.0000\n",
       "  0.0000\n",
       "  5.5570\n",
       "  0.0000\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MixedInputModel.forward of MixedInputModel(\n",
       "  (embs): ModuleList(\n",
       "    (0): Embedding(1503425, 50)\n",
       "    (1): Embedding(771770, 50)\n",
       "    (2): Embedding(29, 15)\n",
       "    (3): Embedding(1734, 50)\n",
       "    (4): Embedding(10, 5)\n",
       "    (5): Embedding(48, 24)\n",
       "    (6): Embedding(372, 50)\n",
       "    (7): Embedding(272, 50)\n",
       "    (8): Embedding(1220, 50)\n",
       "    (9): Embedding(788378, 50)\n",
       "    (10): Embedding(1317103, 50)\n",
       "    (11): Embedding(4, 2)\n",
       "  )\n",
       "  (lins): ModuleList(\n",
       "    (0): Linear(in_features=451, out_features=1000, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (outp): Linear(in_features=500, out_features=1, bias=True)\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (drops): ModuleList(\n",
       "    (0): Dropout(p=0.001)\n",
       "    (1): Dropout(p=0.01)\n",
       "  )\n",
       "  (bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True)\n",
       ")>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = m.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(1503425, 50),\n",
       " Embedding(771770, 50),\n",
       " Embedding(29, 15),\n",
       " Embedding(1734, 50),\n",
       " Embedding(10, 5),\n",
       " Embedding(48, 24),\n",
       " Embedding(372, 50),\n",
       " Embedding(272, 50),\n",
       " Embedding(1220, 50),\n",
       " Embedding(788378, 50),\n",
       " Embedding(1317103, 50),\n",
       " Embedding(4, 2)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for i, e in enumerate(model.embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-7a0b8816cd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-7a0b8816cd7d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)"
     ]
    }
   ],
   "source": [
    "[e(x_cat[:,i]) for i, e in enumerate(model.embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 5 \n",
       " 7.2742e+05  5.2266e+05  2.2000e+01  5.8600e+02  8.0000e+00  3.5000e+01\n",
       " 9.9214e+05  1.2332e+05  2.0000e+01  4.6100e+02  5.0000e+00  3.0000e+01\n",
       " 3.3169e+05  5.6759e+05  1.6000e+01  1.1540e+03  6.0000e+00  1.4000e+01\n",
       " 1.2928e+06  1.0392e+05  1.0000e+00  1.2400e+02  5.0000e+00  3.0000e+01\n",
       "\n",
       "Columns 6 to 11 \n",
       " 2.6900e+02  2.6800e+02  0.0000e+00  2.5622e+05  1.2115e+06  2.0000e+00\n",
       " 1.2600e+02  1.9300e+02  1.1700e+02  5.5357e+05  5.4308e+05  2.0000e+00\n",
       " 2.5800e+02  2.0800e+02  0.0000e+00  7.2841e+05  1.1293e+06  1.0000e+00\n",
       " 1.2600e+02  1.9300e+02  1.1500e+02  2.0214e+05  7.3034e+05  2.0000e+00\n",
       "[torch.LongTensor of size 4x12]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0580 -0.1317  1.6634 -0.2437 -0.2929\n",
       "-0.0579 -0.1308 -0.7836 -0.2437 -0.2929\n",
       "-0.0523 -0.1200 -0.1735 -0.2437  3.4144\n",
       "-0.0576 -0.1015 -0.7836 -0.2437 -0.2929\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-96729c863c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)"
     ]
    }
   ],
   "source": [
    "e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-3477ff12a182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.cuda.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.LongTensor\u001b[0m), but expected (torch.cuda.FloatTensor source, int dim, torch.cuda.LongTensor index)"
     ]
    }
   ],
   "source": [
    "e(Variable(x_cat)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1503425, 50)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
