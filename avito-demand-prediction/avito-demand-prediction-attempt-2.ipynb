{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.preprocessing import CategoricalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torchvision.models as models\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os.path as dpath\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "np.set_printoptions(threshold=50, edgeitems=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/edwin/Datasets/competitions/avito-demand-prediction/\"\n",
    "PATH = Path(path)\n",
    "TRN_CSV = pd.read_csv(PATH/'train.csv')\n",
    "train, valid = train_test_split(TRN_CSV, train_size=300, test_size=10)\n",
    "sample = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? ColumnarModelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? np.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array for every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c.values for n, c in train.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [c.values for n, c in train.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(columns, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df(train, 'deal_probability', do_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.user_type.astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = []\n",
    "contin_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['item_id', 'user_id', 'region', 'city', 'parent_category_name', 'category_name',\n",
    "           'param_1', 'param_2', 'param_3', 'title', 'description', 'user_type','image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contin_vars = ['price', 'item_seq_number', 'image_top_1', 'deal_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in cat_vars:\n",
    "    df_train_copy[v] = df_train_copy[v].astype('category').cat.as_ordered() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in contin_vars:\n",
    "    df_train_copy[v] = df_train_copy[v].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df(df_train_copy, 'deal_probability', do_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train_copy.drop(['activation_date', 'image'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df(df_train_copy, 'deal_probability', do_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting categories\n",
    "df_train_copy.user_type.cat.categories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "df_train_copy.user_type.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c.values for n, c in df_train_copy.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_stack = np.stack(cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn Category Variables into Cats\n",
    "# Create Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_CSV_COPY = TRN_CSV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_CSV_COPY.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_CSV_COPY.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = TRN_CSV\n",
    "CSV_COPY = CSV.copy()\n",
    "cat_vars = ['item_id', 'user_id', 'region', 'city', 'parent_category_name', \n",
    "            'category_name', 'param_1', 'param_2', 'param_3', 'title', 'description',\n",
    "            'user_type']\n",
    "contin_vars = ['price', 'item_seq_number', 'image_top_1']\n",
    "drop_vars = ['activation_date', 'image']\n",
    "y_vars = ['deal_probability']\n",
    "for v in cat_vars:\n",
    "    CSV_COPY[v] = CSV_COPY[v].astype('category').cat.as_ordered()\n",
    "for v in contin_vars:\n",
    "    CSV_COPY[v] = CSV_COPY[v].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = CSV_COPY[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in sample:\n",
    "    if sample.dtypes[column] == 'category':\n",
    "        print(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in sample:\n",
    "    if hasattr(sample[column], 'cat'):\n",
    "        sample[column] = sample[column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.drop(['activation_date','image'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in sample:\n",
    "    sample[v] = sample[v].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_stack = [c.values for n, c in sample.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack(cols_stack, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.delete(data, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(data, 15, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sample.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? proc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvitoDataset(Dataset):\n",
    "    def __init__(self, CSV):\n",
    "        return \"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRN_CSV_COPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvitoDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.data[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = AvitoDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvitoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvitoModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AvitoModel(\n",
       "  (fc1): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AvitoModel()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample.price.values\n",
    "y = sample.deal_probability.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(X, norm=’l2’,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09564],\n",
       "       [ 0.2    ],\n",
       "       [ 0.2    ],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.05499],\n",
       "       [ 0.13431],\n",
       "       [ 0.     ],\n",
       "       [ 0.20406],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.78503],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.19917],\n",
       "       [ 0.     ],\n",
       "       [ 0.7376 ],\n",
       "       [ 0.     ],\n",
       "       [ 0.61754],\n",
       "       ..., \n",
       "       [ 0.12981],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.86521],\n",
       "       [ 0.4    ],\n",
       "       [ 0.09634],\n",
       "       [ 0.18706],\n",
       "       [ 0.80323],\n",
       "       [ 0.12152],\n",
       "       [ 0.     ],\n",
       "       [ 0.     ],\n",
       "       [ 0.76786],\n",
       "       [ 0.02444],\n",
       "       [ 0.80323],\n",
       "       [ 0.86521],\n",
       "       [ 0.78503],\n",
       "       [ 0.73757],\n",
       "       [ 0.     ]], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       ..., \n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(x_train, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "y_train = np.array(y, dtype=np.float32)\n",
    "\n",
    "x_train = x_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: nan.\n",
      "Iteration: 2. Loss: nan.\n",
      "Iteration: 3. Loss: nan.\n",
      "Iteration: 4. Loss: nan.\n",
      "Iteration: 5. Loss: nan.\n",
      "Iteration: 6. Loss: nan.\n",
      "Iteration: 7. Loss: nan.\n",
      "Iteration: 8. Loss: nan.\n",
      "Iteration: 9. Loss: nan.\n",
      "Iteration: 10. Loss: nan.\n",
      "Iteration: 11. Loss: nan.\n",
      "Iteration: 12. Loss: nan.\n",
      "Iteration: 13. Loss: nan.\n",
      "Iteration: 14. Loss: nan.\n",
      "Iteration: 15. Loss: nan.\n",
      "Iteration: 16. Loss: nan.\n",
      "Iteration: 17. Loss: nan.\n",
      "Iteration: 18. Loss: nan.\n",
      "Iteration: 19. Loss: nan.\n",
      "Iteration: 20. Loss: nan.\n",
      "Iteration: 21. Loss: nan.\n",
      "Iteration: 22. Loss: nan.\n",
      "Iteration: 23. Loss: nan.\n",
      "Iteration: 24. Loss: nan.\n",
      "Iteration: 25. Loss: nan.\n",
      "Iteration: 26. Loss: nan.\n",
      "Iteration: 27. Loss: nan.\n",
      "Iteration: 28. Loss: nan.\n",
      "Iteration: 29. Loss: nan.\n",
      "Iteration: 30. Loss: nan.\n",
      "Iteration: 31. Loss: nan.\n",
      "Iteration: 32. Loss: nan.\n",
      "Iteration: 33. Loss: nan.\n",
      "Iteration: 34. Loss: nan.\n",
      "Iteration: 35. Loss: nan.\n",
      "Iteration: 36. Loss: nan.\n",
      "Iteration: 37. Loss: nan.\n",
      "Iteration: 38. Loss: nan.\n",
      "Iteration: 39. Loss: nan.\n",
      "Iteration: 40. Loss: nan.\n",
      "Iteration: 41. Loss: nan.\n",
      "Iteration: 42. Loss: nan.\n",
      "Iteration: 43. Loss: nan.\n",
      "Iteration: 44. Loss: nan.\n",
      "Iteration: 45. Loss: nan.\n",
      "Iteration: 46. Loss: nan.\n",
      "Iteration: 47. Loss: nan.\n",
      "Iteration: 48. Loss: nan.\n",
      "Iteration: 49. Loss: nan.\n",
      "Iteration: 50. Loss: nan.\n",
      "Iteration: 51. Loss: nan.\n",
      "Iteration: 52. Loss: nan.\n",
      "Iteration: 53. Loss: nan.\n",
      "Iteration: 54. Loss: nan.\n",
      "Iteration: 55. Loss: nan.\n",
      "Iteration: 56. Loss: nan.\n",
      "Iteration: 57. Loss: nan.\n",
      "Iteration: 58. Loss: nan.\n",
      "Iteration: 59. Loss: nan.\n",
      "Iteration: 60. Loss: nan.\n",
      "Iteration: 61. Loss: nan.\n",
      "Iteration: 62. Loss: nan.\n",
      "Iteration: 63. Loss: nan.\n",
      "Iteration: 64. Loss: nan.\n",
      "Iteration: 65. Loss: nan.\n",
      "Iteration: 66. Loss: nan.\n",
      "Iteration: 67. Loss: nan.\n",
      "Iteration: 68. Loss: nan.\n",
      "Iteration: 69. Loss: nan.\n",
      "Iteration: 70. Loss: nan.\n",
      "Iteration: 71. Loss: nan.\n",
      "Iteration: 72. Loss: nan.\n",
      "Iteration: 73. Loss: nan.\n",
      "Iteration: 74. Loss: nan.\n",
      "Iteration: 75. Loss: nan.\n",
      "Iteration: 76. Loss: nan.\n",
      "Iteration: 77. Loss: nan.\n",
      "Iteration: 78. Loss: nan.\n",
      "Iteration: 79. Loss: nan.\n",
      "Iteration: 80. Loss: nan.\n",
      "Iteration: 81. Loss: nan.\n",
      "Iteration: 82. Loss: nan.\n",
      "Iteration: 83. Loss: nan.\n",
      "Iteration: 84. Loss: nan.\n",
      "Iteration: 85. Loss: nan.\n",
      "Iteration: 86. Loss: nan.\n",
      "Iteration: 87. Loss: nan.\n",
      "Iteration: 88. Loss: nan.\n",
      "Iteration: 89. Loss: nan.\n",
      "Iteration: 90. Loss: nan.\n",
      "Iteration: 91. Loss: nan.\n",
      "Iteration: 92. Loss: nan.\n",
      "Iteration: 93. Loss: nan.\n",
      "Iteration: 94. Loss: nan.\n",
      "Iteration: 95. Loss: nan.\n",
      "Iteration: 96. Loss: nan.\n",
      "Iteration: 97. Loss: nan.\n",
      "Iteration: 98. Loss: nan.\n",
      "Iteration: 99. Loss: nan.\n",
      "Iteration: 100. Loss: nan.\n",
      "Iteration: 101. Loss: nan.\n",
      "Iteration: 102. Loss: nan.\n",
      "Iteration: 103. Loss: nan.\n",
      "Iteration: 104. Loss: nan.\n",
      "Iteration: 105. Loss: nan.\n",
      "Iteration: 106. Loss: nan.\n",
      "Iteration: 107. Loss: nan.\n",
      "Iteration: 108. Loss: nan.\n",
      "Iteration: 109. Loss: nan.\n",
      "Iteration: 110. Loss: nan.\n",
      "Iteration: 111. Loss: nan.\n",
      "Iteration: 112. Loss: nan.\n",
      "Iteration: 113. Loss: nan.\n",
      "Iteration: 114. Loss: nan.\n",
      "Iteration: 115. Loss: nan.\n",
      "Iteration: 116. Loss: nan.\n",
      "Iteration: 117. Loss: nan.\n",
      "Iteration: 118. Loss: nan.\n",
      "Iteration: 119. Loss: nan.\n",
      "Iteration: 120. Loss: nan.\n",
      "Iteration: 121. Loss: nan.\n",
      "Iteration: 122. Loss: nan.\n",
      "Iteration: 123. Loss: nan.\n",
      "Iteration: 124. Loss: nan.\n",
      "Iteration: 125. Loss: nan.\n",
      "Iteration: 126. Loss: nan.\n",
      "Iteration: 127. Loss: nan.\n",
      "Iteration: 128. Loss: nan.\n",
      "Iteration: 129. Loss: nan.\n",
      "Iteration: 130. Loss: nan.\n",
      "Iteration: 131. Loss: nan.\n",
      "Iteration: 132. Loss: nan.\n",
      "Iteration: 133. Loss: nan.\n",
      "Iteration: 134. Loss: nan.\n",
      "Iteration: 135. Loss: nan.\n",
      "Iteration: 136. Loss: nan.\n",
      "Iteration: 137. Loss: nan.\n",
      "Iteration: 138. Loss: nan.\n",
      "Iteration: 139. Loss: nan.\n",
      "Iteration: 140. Loss: nan.\n",
      "Iteration: 141. Loss: nan.\n",
      "Iteration: 142. Loss: nan.\n",
      "Iteration: 143. Loss: nan.\n",
      "Iteration: 144. Loss: nan.\n",
      "Iteration: 145. Loss: nan.\n",
      "Iteration: 146. Loss: nan.\n",
      "Iteration: 147. Loss: nan.\n",
      "Iteration: 148. Loss: nan.\n",
      "Iteration: 149. Loss: nan.\n",
      "Iteration: 150. Loss: nan.\n",
      "Iteration: 151. Loss: nan.\n",
      "Iteration: 152. Loss: nan.\n",
      "Iteration: 153. Loss: nan.\n",
      "Iteration: 154. Loss: nan.\n",
      "Iteration: 155. Loss: nan.\n",
      "Iteration: 156. Loss: nan.\n",
      "Iteration: 157. Loss: nan.\n",
      "Iteration: 158. Loss: nan.\n",
      "Iteration: 159. Loss: nan.\n",
      "Iteration: 160. Loss: nan.\n",
      "Iteration: 161. Loss: nan.\n",
      "Iteration: 162. Loss: nan.\n",
      "Iteration: 163. Loss: nan.\n",
      "Iteration: 164. Loss: nan.\n",
      "Iteration: 165. Loss: nan.\n",
      "Iteration: 166. Loss: nan.\n",
      "Iteration: 167. Loss: nan.\n",
      "Iteration: 168. Loss: nan.\n",
      "Iteration: 169. Loss: nan.\n",
      "Iteration: 170. Loss: nan.\n",
      "Iteration: 171. Loss: nan.\n",
      "Iteration: 172. Loss: nan.\n",
      "Iteration: 173. Loss: nan.\n",
      "Iteration: 174. Loss: nan.\n",
      "Iteration: 175. Loss: nan.\n",
      "Iteration: 176. Loss: nan.\n",
      "Iteration: 177. Loss: nan.\n",
      "Iteration: 178. Loss: nan.\n",
      "Iteration: 179. Loss: nan.\n",
      "Iteration: 180. Loss: nan.\n",
      "Iteration: 181. Loss: nan.\n",
      "Iteration: 182. Loss: nan.\n",
      "Iteration: 183. Loss: nan.\n",
      "Iteration: 184. Loss: nan.\n",
      "Iteration: 185. Loss: nan.\n",
      "Iteration: 186. Loss: nan.\n",
      "Iteration: 187. Loss: nan.\n",
      "Iteration: 188. Loss: nan.\n",
      "Iteration: 189. Loss: nan.\n",
      "Iteration: 190. Loss: nan.\n",
      "Iteration: 191. Loss: nan.\n",
      "Iteration: 192. Loss: nan.\n",
      "Iteration: 193. Loss: nan.\n",
      "Iteration: 194. Loss: nan.\n",
      "Iteration: 195. Loss: nan.\n",
      "Iteration: 196. Loss: nan.\n",
      "Iteration: 197. Loss: nan.\n",
      "Iteration: 198. Loss: nan.\n",
      "Iteration: 199. Loss: nan.\n",
      "Iteration: 200. Loss: nan.\n",
      "Iteration: 201. Loss: nan.\n",
      "Iteration: 202. Loss: nan.\n",
      "Iteration: 203. Loss: nan.\n",
      "Iteration: 204. Loss: nan.\n",
      "Iteration: 205. Loss: nan.\n",
      "Iteration: 206. Loss: nan.\n",
      "Iteration: 207. Loss: nan.\n",
      "Iteration: 208. Loss: nan.\n",
      "Iteration: 209. Loss: nan.\n",
      "Iteration: 210. Loss: nan.\n",
      "Iteration: 211. Loss: nan.\n",
      "Iteration: 212. Loss: nan.\n",
      "Iteration: 213. Loss: nan.\n",
      "Iteration: 214. Loss: nan.\n",
      "Iteration: 215. Loss: nan.\n",
      "Iteration: 216. Loss: nan.\n",
      "Iteration: 217. Loss: nan.\n",
      "Iteration: 218. Loss: nan.\n",
      "Iteration: 219. Loss: nan.\n",
      "Iteration: 220. Loss: nan.\n",
      "Iteration: 221. Loss: nan.\n",
      "Iteration: 222. Loss: nan.\n",
      "Iteration: 223. Loss: nan.\n",
      "Iteration: 224. Loss: nan.\n",
      "Iteration: 225. Loss: nan.\n",
      "Iteration: 226. Loss: nan.\n",
      "Iteration: 227. Loss: nan.\n",
      "Iteration: 228. Loss: nan.\n",
      "Iteration: 229. Loss: nan.\n",
      "Iteration: 230. Loss: nan.\n",
      "Iteration: 231. Loss: nan.\n",
      "Iteration: 232. Loss: nan.\n",
      "Iteration: 233. Loss: nan.\n",
      "Iteration: 234. Loss: nan.\n",
      "Iteration: 235. Loss: nan.\n",
      "Iteration: 236. Loss: nan.\n",
      "Iteration: 237. Loss: nan.\n",
      "Iteration: 238. Loss: nan.\n",
      "Iteration: 239. Loss: nan.\n",
      "Iteration: 240. Loss: nan.\n",
      "Iteration: 241. Loss: nan.\n",
      "Iteration: 242. Loss: nan.\n",
      "Iteration: 243. Loss: nan.\n",
      "Iteration: 244. Loss: nan.\n",
      "Iteration: 245. Loss: nan.\n",
      "Iteration: 246. Loss: nan.\n",
      "Iteration: 247. Loss: nan.\n",
      "Iteration: 248. Loss: nan.\n",
      "Iteration: 249. Loss: nan.\n",
      "Iteration: 250. Loss: nan.\n",
      "Iteration: 251. Loss: nan.\n",
      "Iteration: 252. Loss: nan.\n",
      "Iteration: 253. Loss: nan.\n",
      "Iteration: 254. Loss: nan.\n",
      "Iteration: 255. Loss: nan.\n",
      "Iteration: 256. Loss: nan.\n",
      "Iteration: 257. Loss: nan.\n",
      "Iteration: 258. Loss: nan.\n",
      "Iteration: 259. Loss: nan.\n",
      "Iteration: 260. Loss: nan.\n",
      "Iteration: 261. Loss: nan.\n",
      "Iteration: 262. Loss: nan.\n",
      "Iteration: 263. Loss: nan.\n",
      "Iteration: 264. Loss: nan.\n",
      "Iteration: 265. Loss: nan.\n",
      "Iteration: 266. Loss: nan.\n",
      "Iteration: 267. Loss: nan.\n",
      "Iteration: 268. Loss: nan.\n",
      "Iteration: 269. Loss: nan.\n",
      "Iteration: 270. Loss: nan.\n",
      "Iteration: 271. Loss: nan.\n",
      "Iteration: 272. Loss: nan.\n",
      "Iteration: 273. Loss: nan.\n",
      "Iteration: 274. Loss: nan.\n",
      "Iteration: 275. Loss: nan.\n",
      "Iteration: 276. Loss: nan.\n",
      "Iteration: 277. Loss: nan.\n",
      "Iteration: 278. Loss: nan.\n",
      "Iteration: 279. Loss: nan.\n",
      "Iteration: 280. Loss: nan.\n",
      "Iteration: 281. Loss: nan.\n",
      "Iteration: 282. Loss: nan.\n",
      "Iteration: 283. Loss: nan.\n",
      "Iteration: 284. Loss: nan.\n",
      "Iteration: 285. Loss: nan.\n",
      "Iteration: 286. Loss: nan.\n",
      "Iteration: 287. Loss: nan.\n",
      "Iteration: 288. Loss: nan.\n",
      "Iteration: 289. Loss: nan.\n",
      "Iteration: 290. Loss: nan.\n",
      "Iteration: 291. Loss: nan.\n",
      "Iteration: 292. Loss: nan.\n",
      "Iteration: 293. Loss: nan.\n",
      "Iteration: 294. Loss: nan.\n",
      "Iteration: 295. Loss: nan.\n",
      "Iteration: 296. Loss: nan.\n",
      "Iteration: 297. Loss: nan.\n",
      "Iteration: 298. Loss: nan.\n",
      "Iteration: 299. Loss: nan.\n",
      "Iteration: 300. Loss: nan.\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    x_ = Variable(torch.from_numpy(x_train).cuda())\n",
    "    y_ = Variable(torch.from_numpy(y_train).cuda())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    outputs = model(x_)\n",
    "        \n",
    "    loss = criterion(outputs, y_)\n",
    "    \n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    iter += 1\n",
    "        \n",
    "    print('Iteration: {}. Loss: {}.'.format(iter, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x/x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[     4000.       500.         0.       600.      3700.   1300000.     50000.      1500.      1700.     22000.\n       900.       200.       500.         0.       380.   1900000.      1500.    330000.       500.         0.\n ...,   1000000.      2000.       700.      2100.       100.      9300.      1000.    650000.         0.\n      1000.       300.       200.      3000.     10000.  41280000.       150.       800.      2000.    268000.\n       500.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-f99aebce7e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1446\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    513\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[     4000.       500.         0.       600.      3700.   1300000.     50000.      1500.      1700.     22000.\n       900.       200.       500.         0.       380.   1900000.      1500.    330000.       500.         0.\n ...,   1000000.      2000.       700.      2100.       100.      9300.      1000.    650000.         0.\n      1000.       300.       200.      3000.     10000.  41280000.       150.       800.      2000.    268000.\n       500.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "normalize(x,norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
